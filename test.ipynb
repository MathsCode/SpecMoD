{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5594668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.487224634804267\n"
     ]
    }
   ],
   "source": [
    "import torch, json\n",
    "start_id = 0\n",
    "end_id = 184\n",
    "path_dir = \"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/global_router/sharegpt\"\n",
    "avg_len = 0\n",
    "for i in range(start_id, end_id):\n",
    "    start_idx = i*100\n",
    "    end_idx = start_idx + 100\n",
    "    json_path = f\"{path_dir}/sharegpt_common_en_Qwen3-8B_normal_info_{start_idx}_{end_idx}.json\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        for id, meta_json_data in data.items():\n",
    "            avg_len += meta_json_data['avg_len']\n",
    "print(avg_len / 18400)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f4bb566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f018486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "class RouterConfig(Dict):\n",
    "    def __getattr__(self, item):\n",
    "        try:\n",
    "            return self[item]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f\"No such attribute: {item}\")\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self[key] = value\n",
    "    \n",
    "a = {'a':2}\n",
    "a = RouterConfig(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3e0f624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e52a413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58434"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/global_router/sharegpt/sharegpt_common_en_Qwen3-8B_normal_info_0_100.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    tot = 0\n",
    "    for id, json_data in data.items():\n",
    "        tot += len(json_data['output'])\n",
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5373b68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bc9d81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/inspire/hdd/global_user/xujiaming-253308120313/anaconda3/envs/specmod/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "Ori_model_path = f\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(Ori_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e149b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/inspire/hdd/global_user/xujiaming-253308120313/anaconda3/envs/deepspeed/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Ori_model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqwen3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_qwen3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Qwen3ForCausalLM\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = Qwen3ForCausalLM.from_pretrained(\u001b[43mOri_model_path\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Ori_model_path' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers.models.qwen3.modeling_qwen3 import Qwen3ForCausalLM\n",
    "model = Qwen3ForCausalLM.from_pretrained(Ori_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cd3beab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(151936, 4096)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "290e6643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2,3,4,5]]).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995940df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23469, 12288])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8dde34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output = []\n",
    "with open(f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/dataset/eval/ShareGPT-Chinese-English-90k/sharegpt_jsonl/computer_en_26k(fixed).jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "    for id, meta_data in enumerate(data):\n",
    "        new_meta_data = {\"question_id\":id, \"category\":meta_data['category'], \"turns\":[meta_data[\"conversation\"][0][\"human\"]], \"reference\":[meta_data[\"conversation\"][0][\"assistant\"]]}\n",
    "        output.append(new_meta_data)\n",
    "\n",
    "\n",
    "with open(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/dataset/benchmark/sharegpt_computer_en/question.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in output:\n",
    "        # 1. json.dumps 将字典转为字符串\n",
    "        # 2. ensure_ascii=False 保证中文正常显示，而不是显示成 \\uXXXX\n",
    "        # 3. + \"\\n\" 手动添加换行符\n",
    "        line = json.dumps(item, ensure_ascii=False)\n",
    "        f.write(line + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b61feee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除第 1142 行：无效的conversation\n",
      "删除第 3728 行：无效的conversation\n",
      "删除第 4276 行：无效的conversation\n",
      "删除第 5898 行：无效的conversation\n",
      "删除第 11126 行：无效的conversation\n",
      "删除第 13178 行：无效的conversation\n",
      "检查完成并删除不符合要求的行。\n"
     ]
    }
   ],
   "source": [
    "# 用于检查数据集的每一条记录是否符合要求，不符合要求的将被删除\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 定义要检查的文件路径\n",
    "file_path = \"/inspire/hdd/project/inference-chip/xujiaming-253308120313/dataset/eval/ShareGPT-Chinese-English-90k/sharegpt_jsonl/computer_en_26k(fixed).jsonl\"\n",
    "# 定义用于存储有效行的列表\n",
    "valid_lines = []\n",
    "\n",
    "# 打开文件进行逐行检查\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line_number, line in enumerate(file, start=1):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if (\n",
    "                    \"conversation\" in data \n",
    "                    and data[\"conversation\"] \n",
    "                    and data[\"conversation\"][0][\"human\"] != \"\"\n",
    "                    and data[\"conversation\"][0][\"assistant\"] != \"\"\n",
    "                    and data[\"conversation\"][0][\"human\"] != \"continue\"\n",
    "                ):\n",
    "                    valid_lines.append(line)\n",
    "                else:\n",
    "                    print(f\"删除第 {line_number} 行：无效的conversation\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"删除第 {line_number} 行：JSON解析错误\")\n",
    "\n",
    "# 删除原始文件\n",
    "os.remove(file_path)\n",
    "\n",
    "# 将有效行写入新文件\n",
    "with open(file_path, \"w\") as file:\n",
    "    for line in valid_lines:\n",
    "        file.write(line + \"\\n\")\n",
    "\n",
    "print(\"检查完成并删除不符合要求的行。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e24a411e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 199])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(json_data['output']).view(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e3732c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([199, 4096])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens(torch.tensor(json_data['output'])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM,Qwen3ForCausalLM\n",
    "# /share/public/public_models/Qwen3-14B/\n",
    "# /share/others/public_models/Qwen3-14B/\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "datasets = ['alpaca', 'gsm8k', 'sum', 'mt-bench','vicuna-bench', 'math_infini']\n",
    "for dataset in datasets:\n",
    "    \n",
    "    with open(f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/router/{dataset}_Qwen3-8B_data_None_None.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        save_output_path = f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/log/{dataset}_Qwen3-8B_data_None_None.json\"\n",
    "        with open(save_output_path, 'w', encoding='utf-8') as f:\n",
    "            for id, json_data in data.items():\n",
    "                print(tokenizer.decode(json_data['output']), file=f)\n",
    "# data = torch.load(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/router/math_infini_Qwen3-8B_last_hidden_states_0_1.pt\")\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c6e58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/inspire/hdd/global_user/xujiaming-253308120313/anaconda3/envs/specmod/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:56<00:00, 11.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM,Qwen3ForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "388ba817",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "            \"hello world\",\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ceb40a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 10\n",
    "b = [1,2,3,4]\n",
    "int(a in b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4840647c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fd0e973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "1 in torch.load(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/layer_router/alpaca_Qwen3-8B_laye_router_Y_idx3_None_None.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b719aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试加载input id供微调adaptor\n",
    "import json\n",
    "\n",
    "datasets = ['alpaca', 'gsm8k', 'sum', 'mt-bench','vicuna-bench', 'math_infini']\n",
    "all_finetune_data = []\n",
    "for dataset in datasets:\n",
    "# dataset = 'alpaca'\n",
    "    with open(f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/adaptor/{dataset}_Qwen3-8B_data_None_None.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        for id, json_data in data.items():\n",
    "            input_ids = json_data['Prompt']\n",
    "            layer_index = []\n",
    "            for token_info in json_data['Token']:\n",
    "                input_ids.append(token_info['input_id'])\n",
    "                layer_index.append(token_info['layer_index'])\n",
    "            finetune_data = {'input_ids':input_ids, 'layer_index':layer_index}\n",
    "            all_finetune_data.append(finetune_data)\n",
    "save_path = f'./train_data/adaptor_finetune/finetune_data.json'\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(all_finetune_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d7f8f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =\"hello world\"\n",
    "a[:a.index(\"world\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "225e57b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones((1, 1,100))\n",
    "b = torch.ones((1, 1,100))\n",
    "torch.nn.functional.cosine_similarity(a,b, dim =-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecdc91f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/global_router/mt-bench_Qwen3-8B_normal_info_0_2.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for id, json_data in data.items():\n",
    "        print(len(json_data['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cf0fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/inspire/hdd/global_user/xujiaming-253308120313/anaconda3/envs/specmod/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM,Qwen3ForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "with open('/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/adaptor_finetune/finetune_data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "936aa1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<|im_end|>\n",
      "<|im_start|>user\n",
      "Darrell and Allen's ages are in the ratio of 7:11. If their total age now is 162, calculate Allen's age 10 years from now.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, let's see. The problem says that Darrell and Allen's ages are in the ratio of 7:11, and their total age now is 162. I need to find Allen's age 10 years from now. Hmm, ratios can sometimes be tricky, but let me break it down step by step.\n",
      "\n",
      "First, ratios. So, the ratio of their ages is 7:11. That means for every 7 parts Darrell is, Allen is 11 parts. So, if I think of their ages as multiples of some common number, let's call it x. So, Darrell's age would be 7x and Allen's age would be 11x. Then, the total of their ages is 7x + 11x, which equals 18x. And the problem says that their total age is 162. So, 18x = 162. Then, solving for x would give me the value of x, which I can then use to find their individual ages.\n",
      "\n",
      "Let me write that down:\n",
      "\n",
      "Darrell's age = 7x  \n",
      "Allen's age = 11x  \n",
      "Total age = 7x + 11x = 18x = 162  \n",
      "So, x = 162 / 18  \n",
      "Calculating that, 162 divided by 18. Let me do that division. 18 times 9 is 162, right? Because 18*10 is 180, which is too big, so 18*9 is 162. So x = 9.\n",
      "\n",
      "Therefore, Darrell's current age is 7x = 7*9 = 63 years old. Allen's current age is 11x = 11*9 = 99 years old. Wait, that seems like a big age difference, but maybe that's okay. Let me check if the total adds up. 63 + 99 = 162. Yes, that's correct. So their current ages are 63 and 99.\n",
      "\n",
      "But the question asks for Allen's age 10 years from now. So, Allen is currently 99, so in 10 years, he'll be 99 + 10 = 109. That seems straightforward. But let me double-check my steps to make sure I didn't make a mistake.\n",
      "\n",
      "First, the ratio 7:11. Total parts 7+11=18. Total age 162. So each part is 162/18=9. So Darrell is 7*9=63, Allen is 11*9=99. Then, 10 years from now, Allen is 99+10=109. That seems correct. \n",
      "\n",
      "Wait, but is there a possibility that the ratio is reversed? Like, maybe Darrell is 11 and Allen is 7? But the problem says \"Darrell and Allen's ages are in the ratio of 7:11\". So the order is Darrell first, then Allen. So Darrell is 7 parts, Allen is 11 parts. So that's correct. \n",
      "\n",
      "Alternatively, maybe the problem is in the interpretation of the ratio. But ratios are typically given in the order mentioned. So if it's Darrell to Allen, then 7:11. So that's correct. \n",
      "\n",
      "Another check: if Allen is 99 now, then 10 years from now he's 109. That seems like a valid answer. Let me think if there's another way to approach this problem. Maybe using algebra without variables. Let's see.\n",
      "\n",
      "Let me denote Darrell's age as D and Allen's age as A. Given that D/A = 7/11, so cross-multiplying gives 11D = 7A. Also, D + A = 162. So we have two equations:\n",
      "\n",
      "1) 11D = 7A  \n",
      "2) D + A = 162\n",
      "\n",
      "We can solve this system of equations. Let me try substitution. From equation 2, D = 162 - A. Substitute into equation 1:\n",
      "\n",
      "11*(162 - A) = 7A  \n",
      "Expanding that: 11*162 - 11A = 7A  \n",
      "Bring terms together: 11*162 = 7A + 11A  \n",
      "Which is 11*162 = 18A  \n",
      "Therefore, A = (11*162)/18\n",
      "\n",
      "Calcul\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data[80]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3999a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_data in data:\n",
    "    train_meta_data = {}\n",
    "    train_meta_data['input_ids'] = meta_data['input_ids']\n",
    "    print('1')\n",
    "    train_meta_data['prompt_len'] = len(train_meta_data['input_ids']) - len(train_meta_data['layer_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eae26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[2]['input_ids']) - len(data[2]['layer_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/router/alpaca_Qwen3-8B_data_None_None.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for id, json_data in data.items():\n",
    "        print(len(json_data['output']))\n",
    "        for idx, token_info in enumerate(json_data['Token']):\n",
    "            print(token_info['layer_index'])\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['1']['Token'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:,-2:-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0bfec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class ShadowAdapter2(nn.Module):\n",
    "    def __init__(self, hidden_dim, bottleneck_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim: 原模型 Hidden Size (e.g., Llama-3-8B is 4096)\n",
    "            bottleneck_dim: 压缩后的维度 (e.g., 64 or 128), 越小越快\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 下投影：把维度压下去\n",
    "        self.down_proj = nn.Linear(hidden_dim, bottleneck_dim, bias=False)\n",
    "        # 激活函数：保持和 Llama 一致 (SiLU)\n",
    "        self.act = nn.SiLU()\n",
    "        # 上投影：把维度升回来\n",
    "        self.up_proj = nn.Linear(bottleneck_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # 【关键技巧】零初始化 (Zero Initialization)\n",
    "        # 让 up_proj 的权重一开始全是 0。\n",
    "        # 这样初始时: Adapter(x) = 0, Output = x + 0 = x (完美透传)\n",
    "        # 训练开始后，它会慢慢学到非 0 的修正值。\n",
    "        \n",
    "        nn.init.zeros_(self.up_proj.weight)\n",
    "        \n",
    "        # down_proj 正常随机初始化即可\n",
    "        nn.init.kaiming_normal_(self.down_proj.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意：这里我们只计算 Delta (残差部分)\n",
    "        # 最终输出应该是 x + adapter(x)，在训练循环里加\n",
    "        return self.up_proj(self.act(self.down_proj(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adaptor = nn.ModuleList([\n",
    "            ShadowAdapter2(4096, 2048) for _ in range(36)\n",
    "        ])\n",
    "adaptor.load_state_dict(torch.load(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/checkpoint/adaptor/2048_finetune/final_adapters.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec69a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor[0].down_proj.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d722d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor = adaptor.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((1,1,4096), dtype=torch.float16)\n",
    "output = adaptor(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM,Qwen3ForCausalLM\n",
    "# /share/public/public_models/Qwen3-14B/\n",
    "# /share/others/public_models/Qwen3-14B/\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(2461)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            {\"role\": \"system\",\n",
    "                \"content\": \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n",
    "        ]\n",
    "prompt = 'How are you?'\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt\n",
    "})\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "nums = [0 for i in range(36)]\n",
    "with open(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/adaptor/mt-bench_Qwen3-8B_data_None_None.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "for id, json_data in data.items():\n",
    "    token = json_data['Token']\n",
    "    for token_data in token:\n",
    "        for idx in range(36):\n",
    "            if idx not in token_data['layer_index']:\n",
    "                nums[idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc571c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc479a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for i in range(36):\n",
    "    data = torch.load(f'./train_data/adaptor/mt-bench_Qwen3-8B_X_idx{i}_None_None.pt')\n",
    "    # print(data.shape[0] == nums[i], end='')\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fcdcb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m data  = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: '>' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "data  = None\n",
    "max(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1edee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['1']['Token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_name = 'Qwen3-8B'\n",
    "dataset = 'alpaca'\n",
    "begin = 'None'\n",
    "end = 'None'\n",
    "cur_hidden_states = torch.load(f'./train_data/{dataset}_{model_name}_cur_hidden_states_{begin}_{end}.pt')\n",
    "fake_last_hidden_states = torch.load(f'./train_data/{dataset}_{model_name}_fake_last_hidden_states_{begin}_{end}.pt')\n",
    "true_last_hidden_states = torch.load(f'./train_data/{dataset}_{model_name}_true_last_hidden_states_{begin}_{end}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_hidden_states = torch.load(f'./train_data/{dataset}_{model_name}_spec_hidden_states.pt')\n",
    "label = torch.load(f'./train_data/{dataset}_{model_name}_label_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_name = 'Qwen3-8B'\n",
    "dataset = 'alpaca'\n",
    "begin = 'None'\n",
    "end = 'None'\n",
    "train_data = torch.load(f'./train_data/{dataset}_{model_name}_train_data.pt')\n",
    "label = torch.load(f'./train_data/{dataset}_{model_name}_label_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape[0] // 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8205c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to(model.device)\n",
    "for i in range(train_data.shape[0]):\n",
    "    last_hidden_states = train_data[i,:,:train_data.shape[-1]//3]\n",
    "    logits = model.lm_head(last_hidden_states)\n",
    "    token = torch.argmax(logits, dim=-1)\n",
    "    embeddings = model.model.embed_tokens(token)\n",
    "    flag = embeddings == train_data[i,:,train_data.shape[-1]//3:train_data.shape[-1]//3 * 2]\n",
    "    if not torch.all(flag):\n",
    "        print(f'{i} Something wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89003c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = train_data[1315,:,:train_data.shape[-1]//3]\n",
    "logits = model.lm_head(last_hidden_states)\n",
    "token = torch.argmax(logits, dim=-1)\n",
    "embeddings = model.model.embed_tokens(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60009e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d09d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[1315,:,train_data.shape[-1]//3:train_data.shape[-1]//3 * 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885874d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:,:,:train_data.shape[-1]//3*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['alpaca', 'gsm8k', 'math_infini', 'mt-bench', 'sum', 'vicuna-bench']\n",
    "models = ['Qwen3-8B', 'Qwen3-14B']\n",
    "dataset = 'alpaca'\n",
    "model\n",
    "spec_hidden_states_path = f'./train_data/{dataset}_{model}_spec_hidden_states.pt'\n",
    "cur_hidden_states_path = f'./train_data/{dataset}_{model}_cur_hidden_states_None_None.pt'\n",
    "last_hidden_states_path = f'./train_data/{dataset}_{model}_true_last_hidden_states_None_None.pt'\n",
    "spec_hidden_states = torch.load(spec_hidden_states_path)\n",
    "cur_hidden_states = torch.load(cur_hidden_states_path)\n",
    "last_hidden_states = torch.load(last_hidden_states_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Qwen3-8B', 'Qwen3-14B']\n",
    "datasets = ['alpaca', 'gsm8k', 'sum', 'mt-bench','vicuna-bench', 'math_infini']\n",
    "with open(f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ca5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc145d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63def39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e39893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "model_name = 'Qwen3-8B'\n",
    "dataset = 'alpaca'\n",
    "begin = 'None'\n",
    "end = 'None'\n",
    "with open(f\"./train_data/{dataset}_{model_name}_data_{begin}_{end}.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['0']['Token'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cd5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['alpaca', 'gsm8k', 'math_infini', 'mt-bench', 'sum', 'vicuna-bench']\n",
    "models = ['Qwen3-8B', 'Qwen3-14B']\n",
    "import os, json\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        train_data_path = f'./train_data/{dataset}_{model}_data_None_None.json'\n",
    "        if os.path.exists(train_data_path):\n",
    "            with open(train_data_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                tot = 0\n",
    "                for key, value in data.items():\n",
    "                    tot += value['avg_len']\n",
    "                print(f'{dataset}_{model}: {tot / len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for key, value in data.items():    \n",
    "    tot += len(value['Token'])\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.tensor([[151667]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.item() == data['0']['Token'][0]['input_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213692cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_id = 0\n",
    "for key, value in data.items():\n",
    "    for idx, token in enumerate(value['Token']):\n",
    "        input_id = token['input_id']\n",
    "        if  input_id != output_id:\n",
    "            print(value['Token'][idx-1])\n",
    "            print(idx, input_id, output_id)\n",
    "        output_id = token['output_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_logits = model.lm_head(fake_last_hidden_states[3])\n",
    "true_logits = model.lm_head(true_last_hidden_states[4])\n",
    "fake_token = torch.argmax(fake_logits, dim=-1)\n",
    "true_token = torch.argmax(true_logits, dim=-1)\n",
    "print(fake_token, true_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a182d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n",
    "]\n",
    "messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How are you\"\n",
    "        })\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfff494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "min_dtype = torch.finfo(torch.float16).min\n",
    "mask = torch.full((1, 117), fill_value=min_dtype, dtype=torch.float16)\n",
    "cache_pos = torch.arange(117,118)\n",
    "dag = torch.arange(117) > cache_pos.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/home/xujiaming/xujiaming/Paper/SpecMoD/data/alpaca_Qwen3-8B_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    lst = []\n",
    "    for key, value in data.items():\n",
    "        lst.append(value['avg_len'])\n",
    "    print(sum(lst) / len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6747cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for key, value in data.items():\n",
    "    lst.append(value['avg_len'])\n",
    "print(sum(lst) / len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/xujiaming/xujiaming/Paper/SpecMoD/data/alpaca_Qwen3-8B_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    lst = []\n",
    "    ground_truth = []\n",
    "    for q_id, q_re in data.items():\n",
    "        for token_id, token_value in q_re['Token'].items():\n",
    "            lst.append(eval(token_id))\n",
    "            ground_truth.append(token_value['layer_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae652b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from model.qwen3_model import Spec_Qwen3ForCausalLM\n",
    "\n",
    "model = Spec_Qwen3ForCausalLM.from_pretrained(f\"/share/others/public_models/Qwen3-14B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6cd660",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data_emb = model.model.embed_tokens(torch.tensor(lst)).detach()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdeba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[[] for i in range(10)] for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "a={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25623ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[100] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0].append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][1] = a[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a90ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "v1 = torch.tensor([1., 2., 3.]).unsqueeze(0).unsqueeze(0)\n",
    "v2 = torch.tensor([1., 1., 3.]).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# 对于 1D 向量，我们需要在 dim=0 上计算\n",
    "similarity_1d = F.cosine_similarity(v1, v2, dim= -1).mean()\n",
    "print(f\"v1: {v1}\")\n",
    "print(f\"v2: {v2}\")\n",
    "print(f\"1D 向量余弦相似度: {similarity_1d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1.,2.,3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b874614",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb653dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class PathPredictorMLP(nn.Module):\n",
    "    def __init__(self, n_layers, llm_hidden_dim, mlp_internal_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = llm_hidden_dim \n",
    "        self.output_dim = n_layers\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, mlp_internal_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(mlp_internal_dim), \n",
    "            nn.Linear(mlp_internal_dim, self.output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "predictor = PathPredictorMLP(\n",
    "    n_layers=36,\n",
    "    llm_hidden_dim=4096,\n",
    "    mlp_internal_dim=1024,)\n",
    "predictor.load_state_dict(torch.load(\"/home/xujiaming/xujiaming/Paper/SpecMoD/path_predictor_mlp_baseline.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 1, 4096)\n",
    "logits = predictor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edede12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127688b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ShadowAdapter(nn.Module):\n",
    "    def __init__(self, hidden_dim, bottleneck_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim: 原模型 Hidden Size (e.g., Llama-3-8B is 4096)\n",
    "            bottleneck_dim: 压缩后的维度 (e.g., 64 or 128), 越小越快\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 下投影：把维度压下去\n",
    "        self.down_proj = nn.Linear(hidden_dim, bottleneck_dim, bias=False)\n",
    "        # 激活函数：保持和 Llama 一致 (SiLU)\n",
    "        self.act = nn.SiLU()\n",
    "        # 上投影：把维度升回来\n",
    "        self.up_proj = nn.Linear(bottleneck_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # 【关键技巧】零初始化 (Zero Initialization)\n",
    "        # 让 up_proj 的权重一开始全是 0。\n",
    "        # 这样初始时: Adapter(x) = 0, Output = x + 0 = x (完美透传)\n",
    "        # 训练开始后，它会慢慢学到非 0 的修正值。\n",
    "        nn.init.zeros_(self.up_proj.weight)\n",
    "        \n",
    "        # down_proj 正常随机初始化即可\n",
    "        nn.init.kaiming_normal_(self.down_proj.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意：这里我们只计算 Delta (残差部分)\n",
    "        # 最终输出应该是 x + adapter(x)，在训练循环里加\n",
    "        return self.up_proj(self.act(self.down_proj(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor = ShadowAdapter(4096, bottleneck_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68febdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/checkpoint/adaptor/adapter_layer_3.pt\")\n",
    "adaptor.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32118883",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor.up_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77192358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "specmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
