{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM,Qwen3ForCausalLM\n",
    "# /share/public/public_models/Qwen3-14B/\n",
    "# /share/others/public_models/Qwen3-14B/\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "datasets = ['alpaca', 'gsm8k', 'sum', 'mt-bench','vicuna-bench', 'math_infini']\n",
    "for dataset in datasets:\n",
    "    \n",
    "    with open(f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/router/{dataset}_Qwen3-8B_data_None_None.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        save_output_path = f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/log/{dataset}_Qwen3-8B_data_None_None.json\"\n",
    "        with open(save_output_path, 'w', encoding='utf-8') as f:\n",
    "            for id, json_data in data.items():\n",
    "                print(tokenizer.decode(json_data['output']), file=f)\n",
    "# data = torch.load(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/router/math_infini_Qwen3-8B_last_hidden_states_0_1.pt\")\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c6e58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/inspire/hdd/global_user/xujiaming-253308120313/anaconda3/envs/specmod/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM,Qwen3ForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd0e973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13824]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(' Wait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b719aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试加载input id供微调adaptor\n",
    "import json\n",
    "\n",
    "datasets = ['alpaca', 'gsm8k', 'sum', 'mt-bench','vicuna-bench', 'math_infini']\n",
    "all_finetune_data = []\n",
    "for dataset in datasets:\n",
    "# dataset = 'alpaca'\n",
    "    with open(f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/adaptor/{dataset}_Qwen3-8B_data_None_None.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        for id, json_data in data.items():\n",
    "            input_ids = json_data['Prompt']\n",
    "            layer_index = []\n",
    "            for token_info in json_data['Token']:\n",
    "                input_ids.append(token_info['input_id'])\n",
    "                layer_index.append(token_info['layer_index'])\n",
    "            finetune_data = {'input_ids':input_ids, 'layer_index':layer_index}\n",
    "            all_finetune_data.append(finetune_data)\n",
    "save_path = f'./train_data/adaptor_finetune/finetune_data.json'\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(all_finetune_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "225e57b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones((1, 1,100))\n",
    "b = torch.ones((1, 1,100))\n",
    "torch.nn.functional.cosine_similarity(a,b, dim =-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/adaptor/{dataset}_Qwen3-8B_data_None_None.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for id, json_data in data.items():\n",
    "        print(len(json_data['Prompt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cf0fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/inspire/hdd/global_user/xujiaming-253308120313/anaconda3/envs/specmod/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM,Qwen3ForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "with open('/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/adaptor_finetune/finetune_data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "936aa1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<|im_end|>\n",
      "<|im_start|>user\n",
      "Darrell and Allen's ages are in the ratio of 7:11. If their total age now is 162, calculate Allen's age 10 years from now.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, let's see. The problem says that Darrell and Allen's ages are in the ratio of 7:11, and their total age now is 162. I need to find Allen's age 10 years from now. Hmm, ratios can sometimes be tricky, but let me break it down step by step.\n",
      "\n",
      "First, ratios. So, the ratio of their ages is 7:11. That means for every 7 parts Darrell is, Allen is 11 parts. So, if I think of their ages as multiples of some common number, let's call it x. So, Darrell's age would be 7x and Allen's age would be 11x. Then, the total of their ages is 7x + 11x, which equals 18x. And the problem says that their total age is 162. So, 18x = 162. Then, solving for x would give me the value of x, which I can then use to find their individual ages.\n",
      "\n",
      "Let me write that down:\n",
      "\n",
      "Darrell's age = 7x  \n",
      "Allen's age = 11x  \n",
      "Total age = 7x + 11x = 18x = 162  \n",
      "So, x = 162 / 18  \n",
      "Calculating that, 162 divided by 18. Let me do that division. 18 times 9 is 162, right? Because 18*10 is 180, which is too big, so 18*9 is 162. So x = 9.\n",
      "\n",
      "Therefore, Darrell's current age is 7x = 7*9 = 63 years old. Allen's current age is 11x = 11*9 = 99 years old. Wait, that seems like a big age difference, but maybe that's okay. Let me check if the total adds up. 63 + 99 = 162. Yes, that's correct. So their current ages are 63 and 99.\n",
      "\n",
      "But the question asks for Allen's age 10 years from now. So, Allen is currently 99, so in 10 years, he'll be 99 + 10 = 109. That seems straightforward. But let me double-check my steps to make sure I didn't make a mistake.\n",
      "\n",
      "First, the ratio 7:11. Total parts 7+11=18. Total age 162. So each part is 162/18=9. So Darrell is 7*9=63, Allen is 11*9=99. Then, 10 years from now, Allen is 99+10=109. That seems correct. \n",
      "\n",
      "Wait, but is there a possibility that the ratio is reversed? Like, maybe Darrell is 11 and Allen is 7? But the problem says \"Darrell and Allen's ages are in the ratio of 7:11\". So the order is Darrell first, then Allen. So Darrell is 7 parts, Allen is 11 parts. So that's correct. \n",
      "\n",
      "Alternatively, maybe the problem is in the interpretation of the ratio. But ratios are typically given in the order mentioned. So if it's Darrell to Allen, then 7:11. So that's correct. \n",
      "\n",
      "Another check: if Allen is 99 now, then 10 years from now he's 109. That seems like a valid answer. Let me think if there's another way to approach this problem. Maybe using algebra without variables. Let's see.\n",
      "\n",
      "Let me denote Darrell's age as D and Allen's age as A. Given that D/A = 7/11, so cross-multiplying gives 11D = 7A. Also, D + A = 162. So we have two equations:\n",
      "\n",
      "1) 11D = 7A  \n",
      "2) D + A = 162\n",
      "\n",
      "We can solve this system of equations. Let me try substitution. From equation 2, D = 162 - A. Substitute into equation 1:\n",
      "\n",
      "11*(162 - A) = 7A  \n",
      "Expanding that: 11*162 - 11A = 7A  \n",
      "Bring terms together: 11*162 = 7A + 11A  \n",
      "Which is 11*162 = 18A  \n",
      "Therefore, A = (11*162)/18\n",
      "\n",
      "Calcul\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(data[80]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3999a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta_data in data:\n",
    "    train_meta_data = {}\n",
    "    train_meta_data['input_ids'] = meta_data['input_ids']\n",
    "    print('1')\n",
    "    train_meta_data['prompt_len'] = len(train_meta_data['input_ids']) - len(train_meta_data['layer_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eae26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[2]['input_ids']) - len(data[2]['layer_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/router/alpaca_Qwen3-8B_data_None_None.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for id, json_data in data.items():\n",
    "        print(len(json_data['output']))\n",
    "        for idx, token_info in enumerate(json_data['Token']):\n",
    "            print(token_info['layer_index'])\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['1']['Token'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:,-2:-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0bfec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class ShadowAdapter2(nn.Module):\n",
    "    def __init__(self, hidden_dim, bottleneck_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim: 原模型 Hidden Size (e.g., Llama-3-8B is 4096)\n",
    "            bottleneck_dim: 压缩后的维度 (e.g., 64 or 128), 越小越快\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 下投影：把维度压下去\n",
    "        self.down_proj = nn.Linear(hidden_dim, bottleneck_dim, bias=False)\n",
    "        # 激活函数：保持和 Llama 一致 (SiLU)\n",
    "        self.act = nn.SiLU()\n",
    "        # 上投影：把维度升回来\n",
    "        self.up_proj = nn.Linear(bottleneck_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # 【关键技巧】零初始化 (Zero Initialization)\n",
    "        # 让 up_proj 的权重一开始全是 0。\n",
    "        # 这样初始时: Adapter(x) = 0, Output = x + 0 = x (完美透传)\n",
    "        # 训练开始后，它会慢慢学到非 0 的修正值。\n",
    "        \n",
    "        nn.init.zeros_(self.up_proj.weight)\n",
    "        \n",
    "        # down_proj 正常随机初始化即可\n",
    "        nn.init.kaiming_normal_(self.down_proj.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意：这里我们只计算 Delta (残差部分)\n",
    "        # 最终输出应该是 x + adapter(x)，在训练循环里加\n",
    "        return self.up_proj(self.act(self.down_proj(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adaptor = nn.ModuleList([\n",
    "            ShadowAdapter2(4096, 2048) for _ in range(36)\n",
    "        ])\n",
    "adaptor.load_state_dict(torch.load(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/checkpoint/adaptor/2048_finetune/final_adapters.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec69a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor[0].down_proj.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d722d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor = adaptor.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((1,1,4096), dtype=torch.float16)\n",
    "output = adaptor(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM,Qwen3ForCausalLM\n",
    "# /share/public/public_models/Qwen3-14B/\n",
    "# /share/others/public_models/Qwen3-14B/\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/inspire/hdd/global_public/public_models/Qwen/Qwen3-8B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(2461)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            {\"role\": \"system\",\n",
    "                \"content\": \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n",
    "        ]\n",
    "prompt = 'How are you?'\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt\n",
    "})\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "nums = [0 for i in range(36)]\n",
    "with open(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/train_data/adaptor/mt-bench_Qwen3-8B_data_None_None.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "for id, json_data in data.items():\n",
    "    token = json_data['Token']\n",
    "    for token_data in token:\n",
    "        for idx in range(36):\n",
    "            if idx not in token_data['layer_index']:\n",
    "                nums[idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc571c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc479a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for i in range(36):\n",
    "    data = torch.load(f'./train_data/adaptor/mt-bench_Qwen3-8B_X_idx{i}_None_None.pt')\n",
    "    # print(data.shape[0] == nums[i], end='')\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcdcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1edee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['1']['Token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_name = 'Qwen3-8B'\n",
    "dataset = 'alpaca'\n",
    "begin = 'None'\n",
    "end = 'None'\n",
    "cur_hidden_states = torch.load(f'./train_data/{dataset}_{model_name}_cur_hidden_states_{begin}_{end}.pt')\n",
    "fake_last_hidden_states = torch.load(f'./train_data/{dataset}_{model_name}_fake_last_hidden_states_{begin}_{end}.pt')\n",
    "true_last_hidden_states = torch.load(f'./train_data/{dataset}_{model_name}_true_last_hidden_states_{begin}_{end}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_hidden_states = torch.load(f'./train_data/{dataset}_{model_name}_spec_hidden_states.pt')\n",
    "label = torch.load(f'./train_data/{dataset}_{model_name}_label_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_name = 'Qwen3-8B'\n",
    "dataset = 'alpaca'\n",
    "begin = 'None'\n",
    "end = 'None'\n",
    "train_data = torch.load(f'./train_data/{dataset}_{model_name}_train_data.pt')\n",
    "label = torch.load(f'./train_data/{dataset}_{model_name}_label_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape[0] // 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8205c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to(model.device)\n",
    "for i in range(train_data.shape[0]):\n",
    "    last_hidden_states = train_data[i,:,:train_data.shape[-1]//3]\n",
    "    logits = model.lm_head(last_hidden_states)\n",
    "    token = torch.argmax(logits, dim=-1)\n",
    "    embeddings = model.model.embed_tokens(token)\n",
    "    flag = embeddings == train_data[i,:,train_data.shape[-1]//3:train_data.shape[-1]//3 * 2]\n",
    "    if not torch.all(flag):\n",
    "        print(f'{i} Something wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89003c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = train_data[1315,:,:train_data.shape[-1]//3]\n",
    "logits = model.lm_head(last_hidden_states)\n",
    "token = torch.argmax(logits, dim=-1)\n",
    "embeddings = model.model.embed_tokens(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60009e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d09d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[1315,:,train_data.shape[-1]//3:train_data.shape[-1]//3 * 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885874d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:,:,:train_data.shape[-1]//3*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['alpaca', 'gsm8k', 'math_infini', 'mt-bench', 'sum', 'vicuna-bench']\n",
    "models = ['Qwen3-8B', 'Qwen3-14B']\n",
    "dataset = 'alpaca'\n",
    "model\n",
    "spec_hidden_states_path = f'./train_data/{dataset}_{model}_spec_hidden_states.pt'\n",
    "cur_hidden_states_path = f'./train_data/{dataset}_{model}_cur_hidden_states_None_None.pt'\n",
    "last_hidden_states_path = f'./train_data/{dataset}_{model}_true_last_hidden_states_None_None.pt'\n",
    "spec_hidden_states = torch.load(spec_hidden_states_path)\n",
    "cur_hidden_states = torch.load(cur_hidden_states_path)\n",
    "last_hidden_states = torch.load(last_hidden_states_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Qwen3-8B', 'Qwen3-14B']\n",
    "datasets = ['alpaca', 'gsm8k', 'sum', 'mt-bench','vicuna-bench', 'math_infini']\n",
    "with open(f'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ca5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc145d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63def39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e39893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "model_name = 'Qwen3-8B'\n",
    "dataset = 'alpaca'\n",
    "begin = 'None'\n",
    "end = 'None'\n",
    "with open(f\"./train_data/{dataset}_{model_name}_data_{begin}_{end}.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['0']['Token'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cd5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['alpaca', 'gsm8k', 'math_infini', 'mt-bench', 'sum', 'vicuna-bench']\n",
    "models = ['Qwen3-8B', 'Qwen3-14B']\n",
    "import os, json\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        train_data_path = f'./train_data/{dataset}_{model}_data_None_None.json'\n",
    "        if os.path.exists(train_data_path):\n",
    "            with open(train_data_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                tot = 0\n",
    "                for key, value in data.items():\n",
    "                    tot += value['avg_len']\n",
    "                print(f'{dataset}_{model}: {tot / len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for key, value in data.items():    \n",
    "    tot += len(value['Token'])\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.tensor([[151667]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.item() == data['0']['Token'][0]['input_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213692cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_id = 0\n",
    "for key, value in data.items():\n",
    "    for idx, token in enumerate(value['Token']):\n",
    "        input_id = token['input_id']\n",
    "        if  input_id != output_id:\n",
    "            print(value['Token'][idx-1])\n",
    "            print(idx, input_id, output_id)\n",
    "        output_id = token['output_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_logits = model.lm_head(fake_last_hidden_states[3])\n",
    "true_logits = model.lm_head(true_last_hidden_states[4])\n",
    "fake_token = torch.argmax(fake_logits, dim=-1)\n",
    "true_token = torch.argmax(true_logits, dim=-1)\n",
    "print(fake_token, true_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a182d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n",
    "]\n",
    "messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How are you\"\n",
    "        })\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfff494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "min_dtype = torch.finfo(torch.float16).min\n",
    "mask = torch.full((1, 117), fill_value=min_dtype, dtype=torch.float16)\n",
    "cache_pos = torch.arange(117,118)\n",
    "dag = torch.arange(117) > cache_pos.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/home/xujiaming/xujiaming/Paper/SpecMoD/data/alpaca_Qwen3-8B_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    lst = []\n",
    "    for key, value in data.items():\n",
    "        lst.append(value['avg_len'])\n",
    "    print(sum(lst) / len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6747cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for key, value in data.items():\n",
    "    lst.append(value['avg_len'])\n",
    "print(sum(lst) / len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/xujiaming/xujiaming/Paper/SpecMoD/data/alpaca_Qwen3-8B_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    lst = []\n",
    "    ground_truth = []\n",
    "    for q_id, q_re in data.items():\n",
    "        for token_id, token_value in q_re['Token'].items():\n",
    "            lst.append(eval(token_id))\n",
    "            ground_truth.append(token_value['layer_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae652b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from model.qwen3_model import Spec_Qwen3ForCausalLM\n",
    "\n",
    "model = Spec_Qwen3ForCausalLM.from_pretrained(f\"/share/others/public_models/Qwen3-14B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6cd660",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data_emb = model.model.embed_tokens(torch.tensor(lst)).detach()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdeba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[[] for i in range(10)] for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "a={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25623ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[100] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0].append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][1] = a[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a90ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "v1 = torch.tensor([1., 2., 3.]).unsqueeze(0).unsqueeze(0)\n",
    "v2 = torch.tensor([1., 1., 3.]).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# 对于 1D 向量，我们需要在 dim=0 上计算\n",
    "similarity_1d = F.cosine_similarity(v1, v2, dim= -1).mean()\n",
    "print(f\"v1: {v1}\")\n",
    "print(f\"v2: {v2}\")\n",
    "print(f\"1D 向量余弦相似度: {similarity_1d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1.,2.,3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b874614",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(a).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb653dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class PathPredictorMLP(nn.Module):\n",
    "    def __init__(self, n_layers, llm_hidden_dim, mlp_internal_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = llm_hidden_dim \n",
    "        self.output_dim = n_layers\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, mlp_internal_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(mlp_internal_dim), \n",
    "            nn.Linear(mlp_internal_dim, self.output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "predictor = PathPredictorMLP(\n",
    "    n_layers=36,\n",
    "    llm_hidden_dim=4096,\n",
    "    mlp_internal_dim=1024,)\n",
    "predictor.load_state_dict(torch.load(\"/home/xujiaming/xujiaming/Paper/SpecMoD/path_predictor_mlp_baseline.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 1, 4096)\n",
    "logits = predictor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edede12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127688b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ShadowAdapter(nn.Module):\n",
    "    def __init__(self, hidden_dim, bottleneck_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim: 原模型 Hidden Size (e.g., Llama-3-8B is 4096)\n",
    "            bottleneck_dim: 压缩后的维度 (e.g., 64 or 128), 越小越快\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 下投影：把维度压下去\n",
    "        self.down_proj = nn.Linear(hidden_dim, bottleneck_dim, bias=False)\n",
    "        # 激活函数：保持和 Llama 一致 (SiLU)\n",
    "        self.act = nn.SiLU()\n",
    "        # 上投影：把维度升回来\n",
    "        self.up_proj = nn.Linear(bottleneck_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # 【关键技巧】零初始化 (Zero Initialization)\n",
    "        # 让 up_proj 的权重一开始全是 0。\n",
    "        # 这样初始时: Adapter(x) = 0, Output = x + 0 = x (完美透传)\n",
    "        # 训练开始后，它会慢慢学到非 0 的修正值。\n",
    "        nn.init.zeros_(self.up_proj.weight)\n",
    "        \n",
    "        # down_proj 正常随机初始化即可\n",
    "        nn.init.kaiming_normal_(self.down_proj.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意：这里我们只计算 Delta (残差部分)\n",
    "        # 最终输出应该是 x + adapter(x)，在训练循环里加\n",
    "        return self.up_proj(self.act(self.down_proj(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor = ShadowAdapter(4096, bottleneck_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68febdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"/inspire/hdd/project/inference-chip/xujiaming-253308120313/Paper/SpecMoD/checkpoint/adaptor/adapter_layer_3.pt\")\n",
    "adaptor.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32118883",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptor.up_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77192358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "specmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
